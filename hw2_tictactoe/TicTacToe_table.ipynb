{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da0f7970-c723-4c07-a9a2-25b5057bdbf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from collections import defaultdict\n",
    "import random\n",
    "import numpy as np\n",
    "import copy\n",
    "from tqdm import tqdm\n",
    "\n",
    "from tic_tac_toe import (TicTacToe,\n",
    "                         get_actions,\n",
    "                         check_n_win,\n",
    "                         board_get_hash, \n",
    "                         board_print,\n",
    "                        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18958f61",
   "metadata": {},
   "source": [
    ">   ## Часть первая: крестики-нолики при помощи Q-обучения\n",
    "> В коде, прилагающемся к последней лекции про обучение с подкреплением, реализован Environment для крестиков-ноликов, в котором можно при инициализации указывать разные размеры доски и условия победы, а также функции для рисования, в том числе с указанием оценки различных действий. С этим окружением все задания и связаны.\n",
    ">    1. Реализуйте обычное (табличное) Q-обучение. Обучите стратегии крестиков и ноликов для доски 3х3.\n",
    ">    2. Попробуйте обучить стратегии крестиков и ноликов для доски 4х4 и/или 5х5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "afa1c683-ec3f-476e-8959-4032ecd75ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = TicTacToe(n_rows=3, n_cols=3, n_win=3)\n",
    "check_win = partial(check_n_win, n_win=env.n_win)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f0d7a4fa-2fd1-49c4-bd54-9dbde9079233",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "╭───┬───┬───╮\n",
      "│   │   │   │ \n",
      "├───┼───┼───┤\n",
      "│   │   │   │ \n",
      "├───┼───┼───┤\n",
      "│   │   │   │ \n",
      "╰───┴───┴───╯\n"
     ]
    }
   ],
   "source": [
    "state = env.reset()\n",
    "board, cur_turn = state\n",
    "board_print(board)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "67a7645d-08ea-4acb-b0eb-16bf23abf654",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Крестики выиграли!\n",
      "╭───┬───┬───╮\n",
      "│ o │   │ x │ \n",
      "├───┼───┼───┤\n",
      "│ o │   │ x │ \n",
      "├───┼───┼───┤\n",
      "│   │   │ x │ \n",
      "╰───┴───┴───╯\n"
     ]
    }
   ],
   "source": [
    "def play_random(env):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        actions = get_actions(state)\n",
    "        random_action = actions[np.random.randint(len(actions))]\n",
    "        state, reward, done = env.step(random_action)\n",
    "        if reward == 1:\n",
    "            print(\"Крестики выиграли!\")\n",
    "        if reward == -1:\n",
    "            print(\"Нолики выиграли!\")\n",
    "    board_print(env.board)\n",
    "play_random(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "37b1a3c8-f73f-487c-9138-22724cc3020e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_action(state, action):\n",
    "    board, cur_turn = state\n",
    "    board = board.copy() \n",
    "    x, y = action\n",
    "    board[x, y] = cur_turn\n",
    "    return board, -cur_turn\n",
    "\n",
    "def state_action_key(state, action):\n",
    "    return board_get_hash(apply_action(state, action)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11cfa193",
   "metadata": {},
   "source": [
    "Агента будем обучать играть против самого себя. Ход противника рассчитывает сам агент, выбирая худший для себя вариант. Для ускорения обучения используем проверку выигрыша для возможного хода."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "291f3dc9-117e-4959-a604-167a723f2c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearningAgent:\n",
    "    def __init__(self, alpha=0.1, gamma=0.99):\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.Q = defaultdict(int)\n",
    "\n",
    "    def get_q(self, state, action):\n",
    "        key = state_action_key(state, action)\n",
    "        if key not in self.Q:\n",
    "            state_o = apply_action(state, action)\n",
    "            if check_win(state_o[0], state[1]):\n",
    "                self.Q[key] = 1\n",
    "        return self.Q[key]\n",
    "        \n",
    "    def set_q(self, state, action, q_new):\n",
    "        key = state_action_key(state, action)\n",
    "        err = q_new - self.Q[key]\n",
    "        self.Q[key] += self.alpha * err\n",
    "        return err\n",
    "        \n",
    "    def update(self, state, action, next_state, reward, done):\n",
    "        if done:\n",
    "            err = self.set_q(state, action, reward)\n",
    "        else:\n",
    "            next_q = max(\n",
    "                self.get_q(next_state, action)\n",
    "                for action in get_actions(next_state)\n",
    "            )\n",
    "            err = self.set_q(state, action, reward + self.gamma * next_q)\n",
    "        return err\n",
    "                  \n",
    "    def opponent_act(self, state_o):\n",
    "        actions_o = get_actions(state_o)\n",
    "        if len(actions_o) == 1:\n",
    "            return actions_o[0]\n",
    "        \n",
    "        states_x = [apply_action(state_o, action_o)\n",
    "                    for action_o in actions_o\n",
    "                   ]\n",
    "        \n",
    "        for i, state_x in enumerate(states_x):\n",
    "            board, x_turn = state_x\n",
    "            if check_win(board, -x_turn):\n",
    "                return actions_o[i]\n",
    "        \n",
    "        max_q_x = [max([self.get_q(state_x, action_x) \n",
    "                        for action_x in get_actions(state_x)\n",
    "                       ])\n",
    "                   for state_x in states_x\n",
    "                  ]\n",
    "        min_idx = np.flatnonzero(max_q_x == np.min(max_q_x))\n",
    "        return actions_o[np.random.choice(min_idx)]\n",
    "        #return actions_o[np.argmin(max_q_x)]\n",
    "         \n",
    "    def act(self, state):\n",
    "        actions = get_actions(state)\n",
    "        q_actions = [self.get_q(state, action) for action in actions]\n",
    "        max_idx = np.flatnonzero(q_actions == np.max(q_actions))\n",
    "        return actions[np.random.choice(max_idx)]\n",
    "        #return actions[np.argmax(q_actions)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ebc30388-f6ff-41b2-87ed-399f895553f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def play(env, agent, opponent, first_action=None):\n",
    "    state_x = env.reset()\n",
    "    for step in range(1, 1000):\n",
    "        if step == 1 and first_action is not None:\n",
    "            action_x = first_action\n",
    "        else:\n",
    "            action_x = agent.act(state_x)\n",
    "        state_o, reward_x, done = env.step(action_x)\n",
    "        if done:\n",
    "            return reward_x, step\n",
    "            \n",
    "        action_o = opponent.opponent_act(state_o)\n",
    "        state_x, reward_o, done = env.step(action_o)  \n",
    "        if done:\n",
    "            return reward_o, step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fcce1e3d-e3f4-41c6-862f-8dd4132c88e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomAgent:\n",
    "    @staticmethod\n",
    "    def act(state):\n",
    "        return random.choice(get_actions(state))\n",
    "    \n",
    "    @staticmethod\n",
    "    def opponent_act(state):\n",
    "        return random.choice(get_actions(state))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d5445a72-d82d-4837-ad9e-98bcdb2fee87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_play(env, agent1, agent2, n=1):\n",
    "    if n==1:\n",
    "        return play(env, agent1, agent2)\n",
    "    else:\n",
    "        rewards, steps = zip(*[play(env, agent1, agent2) for _ in range(n)])\n",
    "        return np.mean(rewards), np.mean(steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2e729dd9-6bd0-4c61-aaa4-78bc4da4ff35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(env, agent1, agent2, n=1):\n",
    "    reward1, step1 = mean_play(env, agent1, agent2, n)\n",
    "    reward2, step2 = mean_play(env, agent2, agent1, n)\n",
    "    return reward1, -reward2, (step1 + step2) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c76fc379-057a-4b1a-9300-a9373bbcf3b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = QLearningAgent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e8cbd812-95ef-49e5-87c8-f05012899021",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 1058/10000 [00:03<00:36, 247.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode = 1000, q_mean_err = 0.20, len(Q)=2604\n",
      "evaluate with random = 0.99, 0.77, 3.38\n",
      "evaluate with prev = 1.00, 1.00, 3.50\n",
      "╭───┬───┬───╮\n",
      "│ o │ x │ x │ \n",
      "├───┼───┼───┤\n",
      "│ x │ o │ o │ \n",
      "├───┼───┼───┤\n",
      "│ x │ o │ x │ \n",
      "╰───┴───┴───╯\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2042/10000 [00:06<00:33, 234.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode = 2000, q_mean_err = 0.10, len(Q)=2641\n",
      "evaluate with random = 1.00, 0.66, 3.47\n",
      "evaluate with prev = 0.00, 0.00, 5.00\n",
      "╭───┬───┬───╮\n",
      "│ x │ o │ x │ \n",
      "├───┼───┼───┤\n",
      "│ o │ x │ x │ \n",
      "├───┼───┼───┤\n",
      "│ o │ x │ o │ \n",
      "╰───┴───┴───╯\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███       | 3060/10000 [00:09<00:29, 237.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode = 3000, q_mean_err = 0.10, len(Q)=2651\n",
      "evaluate with random = 1.00, 0.72, 3.41\n",
      "evaluate with prev = 0.00, 0.00, 5.00\n",
      "╭───┬───┬───╮\n",
      "│ x │ o │ x │ \n",
      "├───┼───┼───┤\n",
      "│ o │ x │ x │ \n",
      "├───┼───┼───┤\n",
      "│ o │ x │ o │ \n",
      "╰───┴───┴───╯\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4049/10000 [00:12<00:25, 230.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode = 4000, q_mean_err = 0.09, len(Q)=2662\n",
      "evaluate with random = 0.99, 0.66, 3.42\n",
      "evaluate with prev = 0.00, 0.00, 5.00\n",
      "╭───┬───┬───╮\n",
      "│ x │ o │ x │ \n",
      "├───┼───┼───┤\n",
      "│ o │ x │ x │ \n",
      "├───┼───┼───┤\n",
      "│ o │ x │ o │ \n",
      "╰───┴───┴───╯\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5035/10000 [00:15<00:25, 195.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode = 5000, q_mean_err = 0.10, len(Q)=2680\n",
      "evaluate with random = 0.99, 0.62, 3.47\n",
      "evaluate with prev = 0.00, 0.00, 5.00\n",
      "╭───┬───┬───╮\n",
      "│ x │ o │ o │ \n",
      "├───┼───┼───┤\n",
      "│ o │ x │ x │ \n",
      "├───┼───┼───┤\n",
      "│ x │ x │ o │ \n",
      "╰───┴───┴───╯\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 6038/10000 [00:18<00:17, 231.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode = 6000, q_mean_err = 0.08, len(Q)=2685\n",
      "evaluate with random = 0.99, 0.71, 3.39\n",
      "evaluate with prev = 0.00, 0.00, 5.00\n",
      "╭───┬───┬───╮\n",
      "│ x │ o │ o │ \n",
      "├───┼───┼───┤\n",
      "│ o │ x │ x │ \n",
      "├───┼───┼───┤\n",
      "│ x │ x │ o │ \n",
      "╰───┴───┴───╯\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 7039/10000 [00:21<00:12, 235.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode = 7000, q_mean_err = 0.08, len(Q)=2686\n",
      "evaluate with random = 1.00, 0.64, 3.42\n",
      "evaluate with prev = 0.00, 0.00, 5.00\n",
      "╭───┬───┬───╮\n",
      "│ x │ o │ o │ \n",
      "├───┼───┼───┤\n",
      "│ o │ x │ x │ \n",
      "├───┼───┼───┤\n",
      "│ x │ x │ o │ \n",
      "╰───┴───┴───╯\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 8045/10000 [00:24<00:08, 239.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode = 8000, q_mean_err = 0.09, len(Q)=2686\n",
      "evaluate with random = 1.00, 0.78, 3.33\n",
      "evaluate with prev = 0.00, 0.00, 5.00\n",
      "╭───┬───┬───╮\n",
      "│ x │ o │ x │ \n",
      "├───┼───┼───┤\n",
      "│ o │ x │ x │ \n",
      "├───┼───┼───┤\n",
      "│ o │ x │ o │ \n",
      "╰───┴───┴───╯\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████ | 9051/10000 [00:27<00:03, 238.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode = 9000, q_mean_err = 0.09, len(Q)=2686\n",
      "evaluate with random = 1.00, 0.77, 3.36\n",
      "evaluate with prev = 0.00, 0.00, 5.00\n",
      "╭───┬───┬───╮\n",
      "│ x │ o │ x │ \n",
      "├───┼───┼───┤\n",
      "│ o │ x │ x │ \n",
      "├───┼───┼───┤\n",
      "│ o │ x │ o │ \n",
      "╰───┴───┴───╯\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:30<00:00, 332.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode = 10000, q_mean_err = 0.08, len(Q)=2687\n",
      "evaluate with random = 0.99, 0.79, 3.37\n",
      "evaluate with prev = 0.00, 0.00, 5.00\n",
      "╭───┬───┬───╮\n",
      "│ x │ o │ o │ \n",
      "├───┼───┼───┤\n",
      "│ o │ x │ x │ \n",
      "├───┼───┼───┤\n",
      "│ x │ x │ o │ \n",
      "╰───┴───┴───╯\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def train(env, agent, n_episodes, n_evaluate, eps=0.1):\n",
    "    prev_agent = copy.deepcopy(agent)\n",
    "    reward_sum = 0\n",
    "    err_sum = 0\n",
    "    for i_episode in tqdm(range(1, n_episodes + 1)):\n",
    "        state_x = env.reset()\n",
    "        err_episode = 0\n",
    "        for step in range(1, 100):\n",
    "\n",
    "            if step == 1:\n",
    "                action_x = random.choice(get_actions(state_x))\n",
    "            else:\n",
    "                action_x = agent.act(state_x)\n",
    "                   \n",
    "            state_o, reward, done = env.step(action_x)\n",
    "            if done:\n",
    "                err = agent.update(state_x, action_x, state_o, reward, done)\n",
    "                err_episode += abs(err)\n",
    "                break\n",
    "                \n",
    "            if eps > random.random():    \n",
    "                action_o = random.choice(get_actions(state_o))\n",
    "            else:\n",
    "                action_o = agent.opponent_act(state_o)\n",
    "            next_state_x, reward, done = env.step(action_o) \n",
    "\n",
    "            err = agent.update(state_x, action_x, next_state_x, reward, done)\n",
    "            err_episode += abs(err)\n",
    "            state_x = next_state_x\n",
    "            if done:\n",
    "                break\n",
    "        reward_sum += reward\n",
    "        err_sum += err_episode / step\n",
    "                \n",
    "        if (i_episode) % n_evaluate == 0:\n",
    "            random.seed(0)\n",
    "            x_reward_random, o_reward_random, steps_random = evaluate(env, agent, RandomAgent, n=100)\n",
    "            x_reward_prev, o_reward_prev, steps_prev = evaluate(env, agent, prev_agent, n=1)\n",
    "            print(f'episode = {i_episode}, q_mean_err = {err_sum / n_evaluate :.2f}, len(Q)={len(agent.Q)}')\n",
    "            print(f'evaluate with random = {x_reward_random:.2f}, {o_reward_random:.2f}, {steps_random:.2f}')\n",
    "            print(f'evaluate with prev = {x_reward_prev:.2f}, {o_reward_prev:.2f}, {steps_prev:.2f}')\n",
    "            prev_agent = copy.deepcopy(agent)\n",
    "            reward_sum = 0\n",
    "            err_sum = 0\n",
    "            play(env, agent, agent)\n",
    "            board_print(env.board)      \n",
    "n_evaluate = 1000\n",
    "train(env, agent, 10 * n_evaluate, n_evaluate)            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9553c6eb-dc97-4b44-bb36-4f33234f5169",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.39, -0.29, 4.21)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(env, RandomAgent, RandomAgent, n=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd3e0b52",
   "metadata": {},
   "source": [
    "Для доски 3х3 всего может быть 3^9 = 19689 состояний. Мы обучились на менее 3000 состояний. При этом агент намного обыгрывает случайную стратегию, сам с собой играет вничью."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b122d0a4",
   "metadata": {},
   "source": [
    "Попробуем обучить агента для доски 4х4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "38cc26bb-79d0-4082-9477-24cd2cdf0ef1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|▉         | 9989/100000 [01:20<10:47, 139.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode = 10000, q_mean_err = 0.17, len(Q)=172081\n",
      "evaluate with random = 1.00, 0.76, 3.35\n",
      "evaluate with prev = 1.00, 1.00, 3.50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 10018/100000 [01:21<38:01, 39.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "╭───┬───┬───┬───╮\n",
      "│   │   │   │   │ \n",
      "├───┼───┼───┼───┤\n",
      "│   │   │   │ o │ \n",
      "├───┼───┼───┼───┤\n",
      "│ x │ x │ x │   │ \n",
      "├───┼───┼───┼───┤\n",
      "│   │ o │   │   │ \n",
      "╰───┴───┴───┴───╯\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|█▉        | 19999/100000 [02:38<10:33, 126.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode = 20000, q_mean_err = 0.05, len(Q)=191469\n",
      "evaluate with random = 1.00, 0.80, 3.42\n",
      "evaluate with prev = 1.00, -1.00, 3.50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|██        | 20012/100000 [02:40<53:07, 25.09it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "╭───┬───┬───┬───╮\n",
      "│   │ o │   │ o │ \n",
      "├───┼───┼───┼───┤\n",
      "│   │ x │   │   │ \n",
      "├───┼───┼───┼───┤\n",
      "│ x │ x │ o │   │ \n",
      "├───┼───┼───┼───┤\n",
      "│   │ x │   │   │ \n",
      "╰───┴───┴───┴───╯\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|██▉       | 29998/100000 [03:58<09:29, 122.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode = 30000, q_mean_err = 0.02, len(Q)=197060\n",
      "evaluate with random = 1.00, 0.84, 3.38\n",
      "evaluate with prev = 1.00, -1.00, 4.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 30%|███       | 30011/100000 [03:59<44:37, 26.14it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "╭───┬───┬───┬───╮\n",
      "│   │ x │   │   │ \n",
      "├───┼───┼───┼───┤\n",
      "│   │   │ x │   │ \n",
      "├───┼───┼───┼───┤\n",
      "│   │ x │ o │ x │ \n",
      "├───┼───┼───┼───┤\n",
      "│ o │   │   │ o │ \n",
      "╰───┴───┴───┴───╯\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|███▉      | 39994/100000 [05:18<07:54, 126.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode = 40000, q_mean_err = 0.00, len(Q)=201448\n",
      "evaluate with random = 1.00, 0.78, 3.42\n",
      "evaluate with prev = 1.00, -1.00, 4.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 40019/100000 [05:20<29:46, 33.58it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "╭───┬───┬───┬───╮\n",
      "│   │ x │ x │ o │ \n",
      "├───┼───┼───┼───┤\n",
      "│   │ x │   │ o │ \n",
      "├───┼───┼───┼───┤\n",
      "│ o │ x │   │   │ \n",
      "├───┼───┼───┼───┤\n",
      "│   │   │   │   │ \n",
      "╰───┴───┴───┴───╯\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|████▉     | 49986/100000 [06:40<06:39, 125.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode = 50000, q_mean_err = 0.00, len(Q)=204991\n",
      "evaluate with random = 1.00, 0.82, 3.38\n",
      "evaluate with prev = 1.00, -1.00, 4.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 50014/100000 [06:42<22:39, 36.78it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "╭───┬───┬───┬───╮\n",
      "│   │ x │   │ o │ \n",
      "├───┼───┼───┼───┤\n",
      "│ x │ x │ o │   │ \n",
      "├───┼───┼───┼───┤\n",
      "│   │ x │   │   │ \n",
      "├───┼───┼───┼───┤\n",
      "│ o │   │   │   │ \n",
      "╰───┴───┴───┴───╯\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|█████▉    | 59995/100000 [08:02<05:11, 128.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode = 60000, q_mean_err = 0.00, len(Q)=208627\n",
      "evaluate with random = 1.00, 0.76, 3.46\n",
      "evaluate with prev = 1.00, -1.00, 4.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 60021/100000 [08:04<19:09, 34.78it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "╭───┬───┬───┬───╮\n",
      "│ o │   │ x │   │ \n",
      "├───┼───┼───┼───┤\n",
      "│ x │ x │   │ o │ \n",
      "├───┼───┼───┼───┤\n",
      "│ x │   │   │   │ \n",
      "├───┼───┼───┼───┤\n",
      "│ o │   │   │   │ \n",
      "╰───┴───┴───┴───╯\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|██████▉   | 69999/100000 [09:23<03:38, 137.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode = 70000, q_mean_err = 0.00, len(Q)=211130\n",
      "evaluate with random = 1.00, 0.82, 3.40\n",
      "evaluate with prev = 1.00, -1.00, 4.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 70%|███████   | 70013/100000 [09:24<16:24, 30.45it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "╭───┬───┬───┬───╮\n",
      "│   │ o │   │   │ \n",
      "├───┼───┼───┼───┤\n",
      "│   │ o │ x │   │ \n",
      "├───┼───┼───┼───┤\n",
      "│   │ x │   │   │ \n",
      "├───┼───┼───┼───┤\n",
      "│ x │ x │ o │   │ \n",
      "╰───┴───┴───┴───╯\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|███████▉  | 79998/100000 [10:43<02:25, 137.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode = 80000, q_mean_err = 0.00, len(Q)=214745\n",
      "evaluate with random = 1.00, 0.78, 3.44\n",
      "evaluate with prev = 1.00, -1.00, 4.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 80024/100000 [10:45<09:02, 36.79it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "╭───┬───┬───┬───╮\n",
      "│   │   │   │   │ \n",
      "├───┼───┼───┼───┤\n",
      "│   │   │ x │   │ \n",
      "├───┼───┼───┼───┤\n",
      "│   │ x │   │ o │ \n",
      "├───┼───┼───┼───┤\n",
      "│ x │ x │ o │ o │ \n",
      "╰───┴───┴───┴───╯\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|████████▉ | 89989/100000 [12:04<01:13, 137.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode = 90000, q_mean_err = 0.00, len(Q)=217947\n",
      "evaluate with random = 1.00, 0.70, 3.41\n",
      "evaluate with prev = 1.00, -1.00, 4.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 90016/100000 [12:05<04:22, 37.99it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "╭───┬───┬───┬───╮\n",
      "│ o │   │   │   │ \n",
      "├───┼───┼───┼───┤\n",
      "│   │ x │   │ o │ \n",
      "├───┼───┼───┼───┤\n",
      "│   │ x │   │   │ \n",
      "├───┼───┼───┼───┤\n",
      "│   │ x │ o │ x │ \n",
      "╰───┴───┴───┴───╯\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 99997/100000 [13:24<00:00, 136.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode = 100000, q_mean_err = 0.00, len(Q)=220828\n",
      "evaluate with random = 1.00, 0.86, 3.46\n",
      "evaluate with prev = 1.00, -1.00, 4.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "100%|██████████| 100000/100000 [13:25<00:00, 124.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "╭───┬───┬───┬───╮\n",
      "│   │   │ o │   │ \n",
      "├───┼───┼───┼───┤\n",
      "│   │   │ x │   │ \n",
      "├───┼───┼───┼───┤\n",
      "│   │ x │   │   │ \n",
      "├───┼───┼───┼───┤\n",
      "│ x │ x │ o │ o │ \n",
      "╰───┴───┴───┴───╯\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "env = TicTacToe(n_rows=4, n_cols=4, n_win=3)\n",
    "check_win = partial(check_n_win, n_win=env.n_win)\n",
    "\n",
    "agent4 = QLearningAgent()\n",
    "n_evaluate = 10000\n",
    "train(env, agent4, 10 * n_evaluate, n_evaluate)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6c085020-2008-4e7d-8d4f-f6d374868785",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.24, -0.12, 5.145)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(env, RandomAgent, RandomAgent, n=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76929b28",
   "metadata": {},
   "source": [
    "Для доски 4х4 всего может быть 3^16 = 43_046_721 состояний. Мы обучились на менее 300тыс состояний. В игре выигрывают крестики."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
